# -*- coding: utf-8 -*-
"""AA_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlvpDp6SIhb6FVm2l4wrrYXLeKJnPDNT
"""

from google.colab import drive
#specify project directory in drive eg /content/drive/NLUProject
# drive.flush_and_unmount()
drive.mount('/content/drive')

import pandas as pd

train = pd.read_csv('/content/drive/MyDrive/AA/AA_cls_train.csv')
test = pd.read_csv('/content/drive/MyDrive/AA/AA_cls_test.csv')

train.shape

test.shape

42513 + 382598



# -*- coding: utf-8 -*-
"""“IMDB Data Pre-Processing and Evaluation.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sfb8BurwihJpKr73YyR-VbhiLJFi-8Xh
"""


#define necessary imports
import time
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from torch.utils.data import Dataset, DataLoader

import gensim

import numpy as np
import tensorflow as tf
from tensorflow import keras

from tensorflow.keras.layers import TextVectorization
import os

from tensorflow.keras.layers import Embedding
from tensorflow.keras import layers

# Evaluation
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
import sklearn.metrics


"""Begin Data Preprocessing Section

Demonstrate how we pre-processed the dataset
"""

# Given a sentence of tokens, return the corresponding indices
def convert_token_to_indices(sentence, word_to_ix):
  indices = []
  for token in sentence:
    # Check if the token is in our vocabularly. If it is, get it's index.
    # If not, get the index for the unknown token.
    if token in word_to_ix:
      index = word_to_ix[token]
    else:
      index = word_to_ix["<unk>"]
    indices.append(index)
  return indices

def data_process(datapath):

  df = pd.read_csv(datapath, header=0)
  for i in range(len(df)):
    int(df['author_id'][i])

  tokens = [str(sent).lower().split() for sent in df['text']]

  vocabulary = set(w for s in tokens for w in s)
  # Add the unknown token to our vocabulary
  vocabulary.add("<unk>")
  # Add the <pad> token to our vocabulary
  vocabulary.add("<pad>")

  ix_to_word = sorted(list(vocabulary))

  # Creating a dictionary to find the index of a given word
  word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}

  for ind, word in enumerate(ix_to_word):
    if word == "<pad>":
      paddingID = ind
    else:
      pass

  text_indices = []
  for i in range(len(tokens)):
    text_indices.append(convert_token_to_indices(tokens[i], word_to_ix))

  df['tokens'] = tokens
  df['tokens_id'] = text_indices

  # hits = 0
  # misses = 0
  # embedding_dim = 100
  # embedding_matrix = np.zeros((len(vocabulary), embedding_dim))

  # # Prepare embedding matrix
  # for word, i in word_to_ix.items():
  #     embedding_vector = embeddings_index.get(word)
  #     if embedding_vector is not None:
  #         # Words not found in embedding index will be all-zeros.
  #         # This includes the representation for "padding" and "OOV"
  #         embedding_matrix[i] = embedding_vector
  #         hits += 1
  #     else:
  #         print(word)
  #         misses += 1
  # print("Converted %d words (%d misses)" % (hits, misses))

  return df, paddingID


train, train_pad_id = data_process('/content/drive/MyDrive/AA/AA_train1.csv')
val, val_pad_id = data_process('/content/drive/MyDrive/AA/AA_val1.csv')
test, test_pad_id = data_process('/content/drive/MyDrive/AA/AA_cls_test.csv')



# # Check the distribution of sequence length

# # send the tokens to a list and get the length of each list
# dfColumnAsList = df['tokens'].tolist()
# listOfListLengths
#  = [len(i) for i in dfColumnAsList]

# print("Mean review length:", np.mean(listOfListLengths))
# print("Standard deviation of review length:",np.std(listOfListLengths))

# #plot the list of review lengths as a boxplot
# fig1, ax1 = plt.subplots()
# ax1.boxplot(listOfListLengths)
# plt.show()

# # get all of the unique words in the data's vocab

# uniqueWordsInData = set(df["tokens"].explode().unique())
# print("Size of dataset vocab:",len(uniqueWordsInData))
# # delete after use
# uniqueWordsInData = None

# define the dataset
class CCATDataset(Dataset):
    """CCAT dataset."""

    # initialize the data in the dataset, pad the sequences to the max length
    def __init__(self, df, pad_id):
        self.df = df
        self.paddingID = pad_id
        self.maxLenForDF = self.getMaximumLengthSequence()
        self.padReturningItems(self.maxLenForDF)
        self.sendListToTensors()

# pad all of the sequences up to a given length
    def padReturningItems(self, lengthToPadTo):
        for index, row in self.df.iterrows():
            # if(index % 100 == 0):
            #   print(index)
            paddingNeeded = (lengthToPadTo - len(row['tokens_id']))
            if(paddingNeeded > 0):
                padData = [self.paddingID] * paddingNeeded
                self.df.at[index, "tokens_id"] = row['tokens_id'] + padData
# convert data that will go to the model into tensors
    def sendListToTensors(self):
        for index, row in self.df.iterrows():
            self.df.at[index, "tokens_id"] = torch.tensor(row['tokens_id'], dtype=torch.int)
            # self.df.at[index, "author_id"] = torch.tensor(row['author_id'], dtype=torch.int)
            # self.df.at[index, "author_id"] = torch.tensor(row['author_id'])

        # for i in range(len(self.df)):
        #   self.df['author_id'][i] = torch.tensor(self.df['author_id'][i])
        self.df['author_id'] = torch.tensor(self.df['author_id'])

    def __len__(self):
        return self.df.shape[0]

    def getMaximumLengthSequence(self):
        dfColumnAsList = self.df['tokens_id'].tolist()
        listOfListLengths = [len(i) for i in dfColumnAsList]
        return max(listOfListLengths)
# on getitem, return the row
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        dfRowToReturn = self.df.iloc[idx]
        dictToReturn = {'input': dfRowToReturn['tokens_id'], 'label': dfRowToReturn['author_id']}
        #print(dictToReturn)
        return dictToReturn
#get the number of unique authors
    def getAuthorCount(self):
        uniqueAuthors = self.df["author_id"].unique()
        uniqueAuthorLength = len(uniqueAuthors)
        return uniqueAuthorLength

# train_dataset = CCATDataset(train, train_pad_id)
# validate_dataset = CCATDataset(val, val_pad_id)
# test_dataset = CCATDataset(test, test_pad_id)

# torch.save(test_dataset, '/content/drive/MyDrive/AA/AV/CCAT50/processed/AA_test.pt')
# torch.save(train_dataset, '/content/drive/MyDrive/AA/AV/CCAT50/processed/AA_train.pt')
# torch.save(validate_dataset, '/content/drive/MyDrive/AA/AV/CCAT50/processed/AA_validate.pt')



# # # load in the datasets
# # train_dataset = torch.load('/content/drive/MyDrive/AA/AV/CCAT50/processed/CCATtrain.pt')
# # validate_dataset = torch.load('/content/drive/MyDrive/AA/AV/CCAT50/processed/CCATvalidate.pt')
# # test_dataset = torch.load('/content/drive/MyDrive/AA/AV/CCAT50/processed/CCATtest.pt')

# # place the datasets in the dataloader

# batchSize = 32
# train_dataloader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)
# test_dataloader = DataLoader(test_dataset, batch_size=batchSize)





# !wget http://nlp.stanford.edu/data/glove.6B.zip
# !unzip -q glove.6B.zip



path_to_glove_file = os.path.join(
    os.path.expanduser("~"), "/content/glove.6B.100d.txt"
)

embeddings_index = {}
with open(path_to_glove_file) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))



vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)
text_ds = tf.data.Dataset.from_tensor_slices(train['text']).batch(32)
vectorizer.adapt(text_ds)

voc = vectorizer.get_vocabulary()
word_index = dict(zip(voc, range(len(voc))))


num_tokens = len(voc) + 2
embedding_dim = 100
hits = 0
misses = 0

# Prepare embedding matrix
embedding_matrix = np.zeros((num_tokens, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        # This includes the representation for "padding" and "OOV"
        embedding_matrix[i] = embedding_vector
        hits += 1
    else:
        # print(word)
        misses += 1
print("Converted %d words (%d misses)" % (hits, misses))

# embedding_matrix.shape



embedding_layer = Embedding(
    num_tokens,
    embedding_dim,
    embeddings_initializer=keras.initializers.Constant(embedding_matrix),
    trainable=False,
)



int_sequences_input = keras.Input(shape=(None,), dtype="int64")
embedded_sequences = embedding_layer(int_sequences_input)
x = layers.Conv1D(128, 5, activation="relu")(embedded_sequences)
x = layers.MaxPooling1D(5)(x)
x = layers.Conv1D(128, 5, activation="relu")(x)
x = layers.MaxPooling1D(5)(x)
x = layers.Conv1D(128, 5, activation="relu")(x)
x = layers.GlobalMaxPooling1D()(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dropout(0.5)(x)
preds = layers.Dense(6, activation="softmax")(x)
model = keras.Model(int_sequences_input, preds)
model.summary()

x_train = vectorizer(np.array([[s] for s in train['text']])).numpy()
x_val = vectorizer(np.array([[s] for s in val['text']])).numpy()

y_train = np.array(train['author_id'])
y_val = np.array(val['author_id'])

model.compile(
    loss="sparse_categorical_crossentropy", optimizer="rmsprop", metrics=["acc"]
)
model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))


# serialize model to JSON
model_json = model.to_json()
with open("/content/drive/MyDrive/AA/model_2.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("/content/drive/MyDrive/AA/model_2.h5")
print("Saved model to disk")

# later...

# # load json and create model
# json_file = open('/content/drive/MyDrive/AA/model.json', 'r')
# loaded_model_json = json_file.read()
# json_file.close()
# loaded_model = model_from_json(loaded_model_json)

# # load weights into new model
# loaded_model.load_weights("/content/drive/MyDrive/AA/model.h5")
# print("Loaded model from disk")

# # evaluate loaded model on test data
# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
# score = loaded_model.evaluate(X, Y, verbose=0)
# print("%s: %.2f%%" % (loaded_model.metrics_names[1], score[1]*100))

# # load json and create model
# json_file = open('/content/drive/MyDrive/AA/model_2.json', 'r')
# loaded_model_json = json_file.read()
# json_file.close()
# model = tensorflow.keras.models.model_from_json(loaded_model_json)

# # load weights into new model
# model.load_weights("/content/drive/MyDrive/AA/model_2.h5")
# print("Loaded model from disk")

# model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])


string_input = keras.Input(shape=(1,), dtype="string")
x = vectorizer(string_input)
preds = model(x)
end_to_end_model = keras.Model(string_input, preds)

correct = 0
top2_acc = 0
top3_acc = 0


# for i in range(5):
for i in range(len(test)):
  label = test['author_id'][i]
  probabilities = end_to_end_model.predict(
    [test['text'][i]])

  # print(probabilities)
  predict = np.argmax(probabilities[0])
  # print(label, predict)

  top2_ind = np.argpartition(probabilities[0], -2)[-2:]
  top3_ind = np.argpartition(probabilities[0], -3)[-3:]
  # top2 = a[ind]

  # top2_ = probabilities[0].argsort()[::-1][:10]
  # top3_ = probabilities[0].argsort()[::-1][:20]

  # print(top2_ind)
  # print(top3_ind)

  if label in top2_ind:
        top2_acc += 1
  if label in top3_ind:
        top3_acc += 1

  # print(top2_acc, top3_acc)

  if label == predict:
    correct += 1

  # print(label, predict)

# accuracy = accuracy_score(test['author_id'], probabilities)
accuracy = correct/len(test)
top2 = top2_acc/len(test)
top3 = top3_acc/len(test)



# proba = end_to_end_model.predict_proba(test['text'])
# top2= metrics.top_k_accuracy_score(test['author_id'], proba, k=2)
# top3 = metrics.top_k_accuracy_score(test['author_id'], proba, k=3)

print(accuracy, top2, top3)

print(accuracy, top2, top3)

# 0.26584809352433375 0.44823936207748216 0.6141650789170371